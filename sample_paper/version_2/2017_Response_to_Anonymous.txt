R4: it is not clear whether this is a position statement wrt semantic publications, or if this is a paper in which the author is presenting a framework. If the former, then the author fails to present a coherent position. If the latter, then the author fails in presenting a framework.
> I've revised extensively the paper in order to clarify the scope of the paper precisely. The particular position I try to defend is that it would be possible to use pure syntactic organisation of the various parts composing a scholarly article for inferring its semantic and rhetorical representation. This aspect has been now presented in the introduction in form of a research question. The (partial) positive answer to this question is basically provided by showing how CISE has been implemented for inferring annotations related to the first four layers (mainly related to the structural characterisation of articles) presented in Section 3.

R4: Beyond these issues, there is the problem of readability. The paper is not clearly written, the English is not acceptable of publication standards, the paper is not well organized, there are typos, problems with punctuation, lack of examples, etc. It is not easy to understand the real scope of this paper, I am guessing the author is really trying to get a first impression about an idea; however, the idea is poorly presented. The problem is also not well discussed and again the paper is very disorganized –this is probably because the author is not clear as to the intention of the paper. The author should also consider the whole publication lifecycle; at the very least the author should somehow consider the publication workflow. The paper lacks scope, it is difficult to read, the English doesn’t make it easy for the reader to understand the paper.
> I apologise for the previous organisation of the text. However, even if they have not been organised appropriately in the first submission of this article, the ideas introduced in this article are the results of several studies I was involved in the past six years, studies that have been tested appropriately on real case scenarios. I've revised the whole article and corrected all the typos and grammatical mistakes I've found. As already said to Reviewer 3, I've spent a lot of effort for revising extensively the text and its organisation, including the addition of several examples and explanation for making the whole discussion more robust. Honestly I had not enough time for asking a native speaker to revise the text at this stage. I'll care to go through a native speaker revision if accepted for publication though.

R4: The author makes quite a few unsubstantiated claims that should be better supported; there is quite a lot of literature that is not referenced in this paper and that is quite relevant. There are also commercial applications that should be analyzed against the ideas presented by the author, e.g. nature science graph, and others that I give in my review.
> A new section (i.e. Section 2) has been added so as to discuss related works that have been presented in the past. However, I preferred to focus on approaches and implementations that aimed at inferring annotations from article sources instead of talking about the existing commercial and non-commercial services and resources which publishes datasets of articles metadata, such as Springer Nature SciGraph or OpenCitations.

R4: The author touches on semantic publications but does not contextualize the work within any publication workflow. How are we getting there? Is this an approach that will work only for new publications that are born within the idea/framework/context that the author is presenting?
> I've revised a bit the introduction and, in particular, the section about the theoretical foundations so as to clarify better this aspect. Honestly, I don't think CISE is applicable only to new publications, since one of the main requisites it has is to have the article described with appropriate hierarchical markup, something that is available since years – e.g. see PubMed Central with JATS, or ScienceDirect with the Elsevier Document Schema.

R4: The author does not seem to be well aware of current publication platforms that are doing many of the things that he is describing; to name but a few: https://science.ai/overview, the work at elife labs with lens and R markdown, semantics for Jupiter notebooks, nature science graph, Cochrane linked data, ZPID linked data. There is Biotea, this is somehow doing just what the author is describing. Also, close to the Biotea experience there is “Semantator: Semantic annotator for converting biomedical text to linked data”. There are more examples of work seeking to add semantics to publications. Some address the problem for existing publications; some other authors are working on solutions for novel publications. Either way, all of that is relevant for the work the author is trying to do.
> I've added a section about related works where I introduced some relevant research works in the area. While all the works cited by the reviewer are important, the main part of them does not explicitly tackle the issue introduced in this paper, i.e. if the pure syntactic organisation of the various parts composing a scholarly article can convey somehow its semantic and rhetorical representation. Thus, instead of focussing on interfaces for allowing a human to annotate article parts, I've preferred to focus on some of the existing automatic mechanisms (based on NLP, OCR, Machine Learning tools) that have been proposed in the past for characterising article components.

R4: There is annotation and NLP written all over the work presented in this paper; something the author is not clear about. For instance, automatic annotation is not perfect, far from it; also, there is a lot ambiguity in domain ontologies –ambiguity that is inherited by the annotations. Has the author considered any of these in his approach? Moreover, if the author is going for reasoning over several annotations from various ontologies then I would like to see a really convincing case. SNOMED and MEDRA, ChEBI and PubCHEM illustrate how this is difficult and may lead to contradictions. A running example could do a lot for this paper. In addition, it is not clear if the author is talking about annotation as NER or annotation using NLP pipelines; in any case, neither one of them gives 100%, so one has to also consider human annotation. If human annotation is involved then what could the task look like? What quality parameters (e.g. inter-annotation agreement) should be considered, will the annotations be part of the semantic layer of the paper or will this be an additional payer somehow attached/related to the paper?
> I've added a lot of examples in all the sections of the article so as to improve the readability and the understandability of the approach. I've also clarified in several points (e.g. in the introduction and in Section 6) that this article and, thus, CISE is not about using NLP or NER tools for inferring annotations to the article parts. Rather, the approach presented uses only the containment relationships that exist between document parts as starting point for inferring multi-layer annotations.

R4: > What is the incentive for the author? Shouldn’t this be the replacement of the typesetting in the publication workflow? Is there a benchmark for tools that can automate the process? Adding the semantics, what advantages? What incentives? When in the publication workflow? Whose work is this? From my experience running a workshop addressing issues in semantics in scientific publications I could see how these are issues that need clarity for everyone. These issues are also related to author's available time, nowbody invests time and effort without first knowing why, what for, and how.
> In the case of the SAVE-SD workshops, the incentive provided to authors for including RDF statements in their HTML submissions was the prize (in euros) for the best HTML submission – that was selected also according to the number of RDF statements there were specified. No other kinds of explicit incentives have been provided by the organisation.

R4: > sure, this has been addressed in the past by many authors but… it has also been said that automatic annotation has quite a few problems. The accuracy of the annotation and also the quality of ontologies and also the fact that these annotations will come from several ontologies and this leads to the problem of reasoning over multiple overlapping ontologies that mostly likely will bear contradictions. If the author is talking about human annotation then again, there are quite a few issues to address and investigate before doing it. Mark2cure has had relative success but they work in an overly scoped domain with an overly scoped annotation task. Also, how are u planning to define the annotation workflows involving humans and software?
> While it is an interesting discussion, this article is not about human annotations, nor about proposing a workflow for enabling human+software annotations of papers. The whole work is focussed in trying to provide some insights for answering the research question presented (in the revision) in the introduction.

R4: > This is really interesting but also poorly charted territory in which lots of authors have not really succeeded in the past. Is the author advocating human annotation for the identification of function of the citation or is this some sort of sentiment analysis kind of automated task?
> I've reworded this point in the revision, since it wasn't clear that I was talking about the automatic identification of citation functions.

R4: > Interesting, I have checked the SPAR ontologies and as ontologies they model some of this. However, I could not find the SPAR extractor suite.
> The SPAR extractor suite is included in the RASH Framework, available at https://github.com/essepuntato/rash. I've added a link to the GitHub repository in the revision.

R4: Fred is limited; it is yet unclear how could FRED be applied to a wider context than that described by the authors.
> As far as I know, FRED has not been developed for processing scholarly article only. It is a flexible tool that can be used, in principle, on any text in order to convert it in Linked Open Data.

R4: > Do u mean different languages as in English, Italian, Spanish? Or do u mean different narrative structures? If the latter then, what are the narrative structures most commonly used in scientific literature? This has already been studied before.
> I've revised that passage so as to clearly referring to languages such as English, Italian, Spanish, etc.

R4: > Once again, the author needs examples everywhere.
> I've reworded the passage and clarified everything by using several examples.

R4: > The whole paragraph is really complicated. As just opinions it is fine. For a position statement paper I would expect to see this very well supported. As a paper in which the author is presenting a framework, this needs a lot of work.
> I've clarified this point is Section 6, where I talk about limitations of the approach.

R4: > Ok. Lets start by saying that strictly speaking these are not hypothesis. Consider writing these a problem statements, research questions or outright hypothesis.
> In the revision, I'm now considering them as conditions for the applicability of CISE.

R4: The body>section>paragraph I don’t understand. Everything else in these points is really arguable and needs better backup in the form of references that really support these as assertions. For instance, “there is not need of having a prior…..” perhaps an example could come in handy. The author should focus on a particular domain, select a well-defined corpus of documents and elaborate from there on.
> I have reworded the whole passage in order to clarify better these points.

R4: > This is, IMHO, the most interesting paragraph of the paper. However, the lack of an example just diminishes its importance. Also, the way it is written makes it seem like a lot of opinions. Once again, if this is a position statement paper I would consider this to be part of the overall position. But, even as a position statement the author needs to give the reader more than just his word for unsubstantiated claims.
> Several examples have been added so as to support the whole discussion.

R4: > This is fine, but… ontologies are just models and data talks lauder than words. So, in order to convince me I need to see data, not just models; along with data I would like to see tested data. If just ontologies then at the very least I would like to see instantiated ontologies so that data tells me how you are right. Once again… I need a running example in this paper, I would like to author to focus on a clear message, I would like this paper to be much better articulated. The large number of self-citations are not helping the author in making a clear case –his own previously published papers may arise further evaluation under the light of his claims in this paper. Also, there are lots of papers that should be cited here and that are missing. His section “from structural patterns to structural semantics” indicates me that it is quite simply easier to map JATS/XML elements to ontologies (bearing in mind minimal ontological commitment) rather than embracing many of the things the author seems to embrace. The text that usually follows the e.g. is not enough explanatory; again a running example could help the author to make his case.
> I've added several examples about how these ontologies have been used to annotate article parts by means of CISE. In particular, we have clarified that CISE does not considering a particular markup language like JATS, but it is flexible and can be used with any markup language (which allows to organise article parts hierarchically) without any prior knowledge about the schema of the language and its vocabulary.

R4: > Yes, like for example?
> I've clarified better this point in Section 6 and in the conclusions.

R4: The tittle reads funny, the author should consider making it closer to the content of the paper. Also, the analogy ised in this paper needs some work. I don’t see how it is relevant.
> I've modified the paper accordingly.
