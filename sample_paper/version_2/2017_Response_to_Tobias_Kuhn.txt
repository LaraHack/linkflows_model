R3: The theoretical links are not convincing
> I've revised entirely the theoretical part, that now is introduced in Section 3. In particular, I've focussed the argument on the principle of compositionality and the downward causation, and I've added examples of applications of both the theories. The Curry-Howard Isomorphism and the Montague grammars are now quickly presented as exemplar applications of the aforementioned theories, without providing any additional detail that can distract the reader from the point of the article.

R3: Lack of discussion on limitations, possible downsides, and assumptions made
> I've added a new section (i.e. Section 6) for discussing the limitations and assumptions of the approach.

R3: I think Section 4 is the most interesting part, but unfortunately only the lowest layers are described in detail (which, in my view, are the least interesting for the main point the paper is making). I suggest to put less focus on the theoretical links and more on the practical experiences and preliminary findings at the higher levels.
> I've added several examples and explanations about all the aspects that have been implemented and tested. However, these implementations refer to the first four layers, that mainly concern the structural characterisations of scholarly articles. In the current shape, such implementations show precisely how the idea depicted by CISE works. In addition, I've added more speculation about future developments of the research in Section 6.

R3: I don't think, however, that the principle of compositionality can really carry all the argumentative weight that is put onto it here. After all, there is no final proof that this principle really holds for natural languages in their entirety. There are in fact many known cases where compositionality doesn't hold, such as for idiomatic expressions or sarcasm. Furthermore, even if we assert the principle of compositionality as a fact, it would only tell us that we could in principle semantically parse papers in an automated fashion, but it would not allow us to conclude that this is feasible in any realistic setting. In fact, over and over again, all kinds of ambitious natural language processing has been proven to be very difficult and often infeasible with current technology.
> It is true that the principle of compositionality alone is not enough for the goal I want to reach with CISE, and I've clarified this position in Section 3. As a consequence, the part about the principle of compositionality has been extended and accompanied by another crucial theory,the downward causation (i.e. the fact that higher layers can cause the specification of new meanings to a lower layer), which was only implicitly cited before. In addition, I've also clarified that, even if the principle of compositionality failed or, in the best case scenario, has shown several drawbacks when considering natural language processing, in this work I explicitly avoid to use NLP technologies for annotating the various article parts. Rather, my approach is entirely based on pure Document Engineering techniques, that consider only the containment relations between the article parts – without caring about the natural language used for writing the articles themselves. Thus, the critiques moved in the past on the use of the principle of compositionality and NLP do not apply in this context.

R3: At what accuracy do you think we can perform such a full parse of scientific papers? Automated approaches are never perfect (often with accuracy levels below 70% for non-trivial NLP tasks), and this seems to heavily affect the arguments made in the paper.
> Honestly, it is difficult to know precisely the answer to that question, since it would be difficult to consider the overall annotations added according to all the layers proposed. However, I've added some information about the precision and recall (~0.85) of the outcomes of the preliminary experiments we have run by using the implementations of CISE introduced in this article.

R3: When do you think we will be able to perform complete semantic analyses of scientific papers? In 5, 10, 50 years from now? What should we be doing until then?
> It is difficult to say, since I have not experimented the use of CISE with the higher layers of annotations yet, and the corpus of documents we have used in the experiments were referring only to a few of communities. Optimistically, the aim would be to implement a good mechanism for retrieving such multi-layer annotations by means of CISE by 5-10 years.

R3: Sometimes authors write in ambiguous sentences (also for human readers), and deliberately or accidentally leave out important information. With your approach, we are stuck with incomplete information in these cases, whereas involving authors in the process could solve this. This shortcoming of the approach is not discussed.
> Actually, CISE does not consider the processing of the natural language text at all by design. Thus, it would not be able to identify the absence of these important information, as well as other aspects related to the pure NLP analysis of the textual content – e.g. the recognition of named entities. I've clarified this point in Section 6, when I speak about the limitations of the approach.

R3: The "iterative" part of the "compositional and iterative semantic enhancement" is not really explained. Is "iterative" referring to applying one layer after the other? To me, this wouldn't be an intuitive use of the term "iterative". I think "iterative" would imply to go through all the layers (or the individual layers) several times.
> I've added several pseudo-codes describing CISE and its implementations. This should clarify why CISE is iterative – it goes through a the set rules several times, until no new annotations are added.

R3: I think "the" in the title should be omitted: "Automating Semantic Publishing" instead of "Automating the Semantic Publishing" (and same for the first sentence of the abstract)
> Corrected.

R3: In general, I suggest to have a native speaker check the document with respect to grammar and style. At several places, I think that some of the used grammar constructs are awkward if not incorrect, but not being a native speaker either, I don't feel confident in my own judgment in what might be borderline cases or simply a matter of taste.
> I've revised the whole article and corrected all the typos and grammatical mistakes I've found. I've spent a lot of effort for revising extensively the text, and honestly I had not enough time for asking a native speaker to revise the text at this stage. I'll care to go through a native speaker revision if the paper will be accepted for publication.

R3: The first paragraph of Section 1 contains many links but no citations (except for the last sentence) that would provide evidence for claims like "... have resulted in ... acceleration of the publishing workflow".
> From my perspective, links are informal kinds of citations, even if it would be not appropriate for them (e.g. Wikipedia articles) to be included in the classic reference section with other scholarly works. However, I've added appropriate statements using the property cito:linksTo so as to formally defining them as citations by means of the Citation Typing Ontology (CiTO).

R3: "... which is very close to the recent proposal of the FAIR principles for scholarly data": Very close in what sense? What are the differences?
> The sentence has been reworded clarifying that FAIR implicitly adopted such Semantic Publishing assumptions.

R3: "generally only a very low number of semantic statements (if none at all) is specified by the authors": Can you be more specific? What are the average/median/maximum values?
> The number of statements in a single paper presented during SAVE-SD workshops was found to range from 24 to 903, yielding a median value of 46 (25th percentile 34, 75th percentile 175). I've added this clarification in the article.

R3: With respect to the paragraphs connecting to Genuine Semantic Publishing, I am not sure whether an average reader is given enough background to understand this discussion. Maybe the issue of "should we or shouldn't we require authors to make a significant extra effort?" could be stated more clearly and more explicitly.
> I've revised the introduction in order to stress more on the fact that, while the authors should make that extra-effort, (s)he must be appropriately supported by at least semi-automatic mechanisms.

R3: "The idea is that the aforementioned approaches can work correctly only if used with documents stored in a particular format ...": Do these *approaches* really only work with a particular format, or is it just the current *implementations* of these approaches? I think this is an important difference.
> I've reworded the sentence in order to make the point clearer. I'm not talking about approaches in that context anymore, but rather I talk about tools, which is more precise.

R3: Contrary to "... if the text to process is written in a particular language such as English, as happens for FRED [9]", I read on the linked website that "FRED is [...] able to parse natural language text in 48 different languages". This should be clarified..
> I've added a footnote that clarify this passage. In particular, even if the official website of FRED claims it is able to “parse natural language text in 48 different languages and transform it to linked data”, from empirical tests it seems it has been trained appropriately only for English text. In fact, the other languages are not addressed directly, but rather they are handled by translating non-English text into English (via the Microsoft Translation API) and then by applying the usual FRED workflow for transforming the English translation into Linked Data.

R3: "It is worth mentioning that this approach is not merely theoretical, but rather it has been implemented ...": An important qualification here is that is has been *partially* implemented. None of these grammar correctly represent an entire natural language.
> I've revised entirely the text in order to explain the whole point better. In particular, CISE has been now presented as a proper algorithm by means of a Python-like pseudo-code, and I've clarified that colleagues and I have provided some implementations of the approach in order to inferring automatically annotations belonging to the first four layers. Then, I've repeated in the conclusions that CISE, and its current implementations, provide only a partial, even if positive, answer to the research question presented in the introduction.

R3: I didn't understand why "hierarchical markup" is needed as an assumption in Section 3. If you assume that natural language sentences can be automatically parsed at great accuracy (as you seem to be assuming), then certainly you can automatically detect the hierarchical structure of documents as well.
> Well, basically it strictly depends on the input format of the document in consideration. The point is that CISE needs appropriate hierarchical marked-up documents for working correctly, and this is feasible to obtain if we consider XML-like markup languages and even LaTeX, but it is surely more difficult if we try to parse, for instance, PDF documents. In Section 6 I've tried to clarify why the need of having appropriate “hierarchical markup” is so important.

R3: "there is no need of having a prior knowledge about the particular natural language used for writing the scholarly article": I don't understand what you mean by "prior knowledge" here. Somebody or something would need some knowledge (in fact deep knowledge) about the language to semantically parse the text at all the layers.
> I've revised the text in order to clarify that CISE does not consider the actual textual content of the article but only the containment relations between its part. This justify the fact that no knowledge about the language used for writing the document content is necessary for using the approach.

R3: Figure 1: I think I understand the meaning of the colors in this figure, but I failed to understand the meaning of the x and y axes. This should be explained better.
> I've entirely revised the figure in order to make it clearer. In addition, it has been also accompanied by additional examples.

R3: Section 4: I would have liked to learn a bit more about the ontologies, tools, and existing studies on the layers 4 to 8.
> I've extended a bit the discussion introducing a particular aspect related to layer 4 in Section 5. However, currently I don't have performed strong studies about the layers 5, 6, 7, and 8 so far, and, thus, it would be difficult to provide certain approaches or details about how to use CISE for inferring the annotations related to those layers. However, I've extended a bit the discussion about them in Section 3 and in Section 6.

R3: Section 4: I would expect some of the most difficult but also most interesting kind of knowledge to extract from a paper to be domain knowledge, i.e. what the authors have found out about the world (e.g. about living organisms in the case of biology). I don't see this aspect anywhere in the 8 presented layers. This seems to be another limitation that is not discussed.
> Several existing applications, such as Named Entity Recognition tools (NER), can be used for addressing successfully this aspect. However, the use of the aforementioned tools and other Natural Language Processing (NLP) technologies are out of the scope of CISE. I've clarified this point in the revision.
